# Core dependencies
python-dotenv>=1.0.0
pandas>=2.0.0
numpy>=1.24.0

# Document processing
PyMuPDF>=1.23.0
openpyxl>=3.1.0
python-docx>=0.8.11
Pillow>=10.0.0
pytesseract>=0.3.10

# AI and ML
openai>=1.0.0
torch>=2.0.0
transformers>=4.30.0
accelerate>=0.20.0
bitsandbytes>=0.41.0

# Email processing
email-validator>=2.0.0

# Utilities
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# Development and testing
pytest>=7.4.0
pytest-cov>=4.1.0

# Added from the code block
sentence-transformers>=2.2.0
pdfplumber>=0.9.0
camelot-py>=0.11.0
selenium>=4.15.0
duckduckgo-search>=4.1.0
serpapi>=0.1.5
chromadb>=0.4.0
faiss-cpu>=1.7.4
qdrant-client>=1.6.0
psycopg2-binary>=2.9.0
langchain>=0.1.0
langchain-openai>=0.0.5
langchain-community>=0.0.10
click>=8.1.0
rich>=13.0.0
pytest-asyncio>=0.21.0

# Web framework for API
fastapi>=0.104.0
uvicorn>=0.24.0

# Development
flake8>=6.0.0
mypy>=1.5.0

import torch

def get_optimal_device():
    """Detect and return the best available device."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():  # Apple Silicon
        return torch.device("mps")
    else:
        return torch.device("cpu")

def get_model_config(device):
    """Get model configuration based on available hardware."""
    if device.type == "cuda":
        return {
            "torch_dtype": torch.float16,  # Use half precision for GPU
            "device_map": "auto",
            "load_in_8bit": False,  # Can enable for memory optimization
        }
    else:
        return {
            "torch_dtype": torch.float32,
            "device_map": None,
            "load_in_8bit": True,  # Use quantization for CPU/MPS
        } 